# -*- coding: utf-8 -*-
import os
import re
import glob
import hashlib
import numpy as np
import pandas as pd
import streamlit as st
from dotenv import load_dotenv

# === RAGìš©: Chroma + Ollama ì„ë² ë”© ===
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings

# === Ollama (ê³µì‹ SDK) for generation ===
import ollama  # OpenAI SDK ë¯¸ì‚¬ìš©

# === í˜¸í™˜ ì„í¬íŠ¸ (splitter / Document) ===
try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except ModuleNotFoundError:
    from langchain.text_splitter import RecursiveCharacterTextSplitter  # fallback

try:
    from langchain_core.documents import Document
except ModuleNotFoundError:
    from langchain.docstore.document import Document  # fallback


# ========= í™˜ê²½ì„¤ì • =========
load_dotenv()

CSV_DEFAULT = os.getenv("CSV_DEFAULT", "test.csv")  # ì˜µì…˜
OLLAMA_BASE = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434").rstrip("/")
OLLAMA_GEN_MODEL = os.getenv("OLLAMA_GEN_MODEL", "llama3.1:8b-instruct-q4_0")
OLLAMA_EMBED_MODEL = os.getenv("OLLAMA_EMBED_MODEL", "mxbai-embed-large")

PERSIST_DIR = os.getenv("PERSIST_DIR", "./chroma_creation")
COLLECTION_NAME = os.getenv("COLLECTION_NAME", None)  # ë¯¸ì§€ì • ì‹œ ìµœì‹  í´ë”ëª…ì—ì„œ ìœ ì¶”

# Ollama í´ë¼ì´ì–¸íŠ¸
client = ollama.Client(host=OLLAMA_BASE)


# ========= ìœ í‹¸ =========
def normalize_ko(s: str) -> str:
    if not s:
        return ""
    s = s.lower()
    s = re.sub(r"[^\uac00-\ud7a3a-z0-9\s]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def tokenize_ko(s: str):
    return re.findall(r"[ê°€-í£a-z0-9]{2,}", normalize_ko(s))

def df_fingerprint(df: pd.DataFrame) -> str:
    parts = []
    cols = set(df.columns)
    title_col = "title" if "title" in cols else None
    content_col = "content" if "content" in cols else None
    for _, row in df.iterrows():
        t = (row.get(title_col, "") or "") if title_col else ""
        c = (row.get(content_col, "") or "") if content_col else ""
        parts.append(t + c)
    return hashlib.sha1("|".join(parts).encode("utf-8")).hexdigest()

def load_csv(csv_path: str):
    if not os.path.isabs(csv_path):
        csv_path = os.path.join(os.getcwd(), csv_path)
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
    df = pd.read_csv(csv_path, dtype=str).fillna("")
    need_cols = {"url", "title", "content", "references", "further_refs"}
    missing = need_cols - set(df.columns)
    if missing:
        raise ValueError(f"CSVì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing}")
    return df

def docs_for_bm25(df: pd.DataFrame):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=800, chunk_overlap=200, length_function=len,
        separators=["\n\n", "\n", " ", ""]
    )
    docs = []
    for ridx, row in df.iterrows():
        title = (row.get("title") or "").strip()
        content = (row.get("content") or "").strip()
        base_text = f"{title}\n\n{content}".strip()
        for cidx, chunk in enumerate(splitter.split_text(base_text)):
            docs.append(Document(
                page_content=chunk,
                metadata={
                    "title": title,
                    "row_id": str(ridx),
                    "chunk_id": f"{ridx}-{cidx}",
                    "url": (row.get("url") or "").strip()
                }
            ))
    return docs

# ë¬¸ìì—´ ì˜ë¼ì£¼ëŠ” í•¨ìˆ˜
def _truncate(text: str, max_chars: int) -> str:
    if len(text) <= max_chars:
        return text
    return text[:max_chars] + "\n\n...[ì¼ë¶€ ìƒëµ]"


# ========= Chroma ì—°ê²° =========
def _pick_latest_collection_dir(base_dir: str) -> str:
    cands = sorted(
        glob.glob(os.path.join(base_dir, "chroma_*")),
        key=os.path.getmtime,
        reverse=True
    )
    if not cands:
        raise RuntimeError(f"Chroma ì»¬ë ‰ì…˜ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {base_dir}")
    return cands[0]

def get_store() -> Chroma:
    emb = OllamaEmbeddings(model=OLLAMA_EMBED_MODEL, base_url=OLLAMA_BASE)
    persist_dir = _pick_latest_collection_dir(PERSIST_DIR)
    col_name = COLLECTION_NAME or os.path.basename(persist_dir).replace("chroma_", "creation_")
    store = Chroma(
        persist_directory=persist_dir,
        collection_name=col_name,
        embedding_function=emb
    )
    return store

def warn_if_embedding_mismatch(store: Chroma):
    try:
        raw_coll = getattr(store, "_collection", None)
        name = getattr(raw_coll, "name", None) or getattr(store, "collection_name", None)
        client_inner = getattr(store, "_client", None)
        if client_inner and hasattr(client_inner, "get_collection"):
            coll = client_inner.get_collection(name)
            meta = getattr(coll, "metadata", None) or {}
        else:
            meta = {}
        idx_model = meta.get("embedding_model")
        if idx_model and idx_model != OLLAMA_EMBED_MODEL:
            st.warning(
                f"âš ï¸ ì´ ì»¬ë ‰ì…˜ì€ ì¸ë±ì‹± ì‹œ '{idx_model}'ë¡œ ì„ë² ë”©ë˜ì—ˆê³ , "
                f"í˜„ì¬ ê²€ìƒ‰ ì„ë² ë”©ì€ '{OLLAMA_EMBED_MODEL}' ì…ë‹ˆë‹¤. "
                f"ê°€ëŠ¥í•˜ë©´ ë™ì¼ ëª¨ë¸ë¡œ ë§ì¶”ì„¸ìš”."
            )
    except Exception:
        pass


# ========= ê²€ìƒ‰ & ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (MMR + í•˜ì´ë¸Œë¦¬ë“œ) =========
def keyword_overlap_score(query: str, text: str) -> float:
    q_toks = set(tokenize_ko(query))
    t_toks = set(tokenize_ko(text))
    if not q_toks:
        return 0.0
    inter = len(q_toks & t_toks)
    return inter / max(3, len(q_toks))

def retrieve_context(
    store: Chroma,
    query: str,
    k: int = 5,
    fetch_k: int = 24,
    ctx_char_limit: int = 4500,
    alpha: float = 0.65,
    use_mmr: bool = True,
    mmr_lambda: float = 0.55
):
    """
    MMR + ì„ë² ë”©/í‚¤ì›Œë“œ í•˜ì´ë¸Œë¦¬ë“œ ì¬ë­í‚¹.
    FAQ ì œëª©(ì˜ë¬¸ë¬¸) ê³¼ë‹¤ ë…¸ì¶œ ì™„í™”.
    """
    if use_mmr and hasattr(store, "max_marginal_relevance_search_with_score"):
        raw = store.max_marginal_relevance_search_with_score(
            query, k=fetch_k, fetch_k=max(fetch_k * 2, 40), lambda_mult=mmr_lambda
        )
    else:
        raw = store.similarity_search_with_score(query, k=fetch_k)

    candidates, seen = [], set()
    for doc, dist in raw:
        key = (doc.metadata.get("chunk_id"), doc.page_content[:120])
        if key in seen:
            continue
        seen.add(key)
        sim = 1.0 / (1.0 + float(dist))
        text_for_kw = f"{doc.metadata.get('title','')}\n{doc.page_content}"
        ko = keyword_overlap_score(query, text_for_kw)
        candidates.append((doc, float(dist), sim, ko))

    def hybrid_score(item):
        _, _, sim, ko = item
        return alpha * sim + (1 - alpha) * ko

    candidates.sort(key=hybrid_score, reverse=True)
    results = candidates[:max(k, 5)]

    ctx_blocks, sources, running_len = [], [], 0
    for doc, dist, sim, ko in results:
        title = (doc.metadata.get("title") or "").strip()
        url = (doc.metadata.get("url") or "").strip()

        block = f"### {title}\n{doc.page_content}"
        if url:
            block += f"\n\n[ì›ë¬¸]({url})"
        block += f"\n\n(ìœ ì‚¬ë„â‰ˆ {sim:.4f} / kwâ‰ˆ {ko:.4f} / distance={dist:.4f})"

        blk = _truncate(block, max_chars=1400)
        if running_len + len(blk) > ctx_char_limit:
            continue
        ctx_blocks.append(blk)
        running_len += len(blk)
        sources.append({"title": title, "url": url, "score": sim, "kw": ko})

    return "\n\n---\n\n".join(ctx_blocks), sources


# ========= ì¦ê±°(ë¬¸ì¥) ì¶”ì¶œ & ì•ˆì „ ìš”ì•½ =========
SENT_SPLIT = re.compile(r"(?<=[.!?])\s+|\n+|[ã€‚ï¼ï¼Ÿã€]\s*")

def build_sentence_pool(context_md: str, max_pool: int = 140):
    """
    ì˜ë¬¸ë¬¸/ë©”ë‰´/í—¤ë” ì œê±° + ì¤‘ë³µ ì œê±°.
    """
    text = re.sub(r"^#+\s.*$", "", context_md, flags=re.MULTILINE)
    sents_raw = [s.strip() for s in SENT_SPLIT.split(text)]
    sents = []
    for s in sents_raw:
        if len(s) < 10 or len(s) > 400:
            continue
        if "ì›ë¬¸](" in s or "ìœ ì‚¬ë„â‰ˆ" in s or "kwâ‰ˆ" in s:
            continue
        if s.endswith("?") or "?" in s:
            continue
        if re.match(r"^[\-â€¢\*]\s", s):
            continue
        sents.append(s)

    uniq, seen = [], set()
    for s in sents:
        key = s[:96]
        if key not in seen:
            uniq.append(s)
            seen.add(key)
        if len(uniq) >= max_pool:
            break
    return uniq

def embed_vecs(texts, emb: OllamaEmbeddings):
    if not texts:
        return []
    return emb.embed_documents(texts)

def cosine(a, b):
    a = np.array(a, dtype=float); b = np.array(b, dtype=float)
    na = np.linalg.norm(a); nb = np.linalg.norm(b)
    if na == 0 or nb == 0: return 0.0
    return float(np.dot(a, b) / (na * nb))

_FACT_HINT = re.compile(r"(ì´ë‹¤|ì˜€ìŠµë‹ˆë‹¤|ì˜€ë‹¤|ì…ë‹ˆë‹¤|ìœ¼ë¡œ|ìœ¼ë¡œì„œ|ìœ¼ë¡œì¨|ë¼ê³ |ëª…|ì„¸|ì‚´|ë…„|ì›”|ì¼|ì¥|ì ˆ|ì¡±ë³´|ì•„ë“¤|ë”¸|ì™•|ê°œ)")
_NUM = re.compile(r"\d")

def rerank_sentences(query: str, sents: list[str], emb: OllamaEmbeddings, top_n: int = 6, beta: float = 0.40):
    """
    ì½”ì‚¬ì¸(ì„ë² ë”©) + í‚¤ì›Œë“œ ê²¹ì¹¨(Î²) + ì‚¬ì‹¤/ìˆ«ì boost, ì˜ë¬¸ë¬¸ penalty
    """
    if not sents:
        return []
    qv = emb.embed_query(query)
    dvs = embed_vecs(sents, emb)

    scored = []
    for s, dv in zip(sents, dvs):
        sim = cosine(qv, dv)
        ko = keyword_overlap_score(query, s)
        boost = 1.0
        if _FACT_HINT.search(s):
            boost += 0.08
        if _NUM.search(s):
            boost += 0.05
        if "?" in s:
            boost -= 0.30
        score = (beta * ko + (1 - beta) * sim) * boost
        scored.append((score, s, sim, ko))

    scored.sort(key=lambda x: x[0], reverse=True)
    return scored[:top_n]

def allowed_token_set(evidence_text: str):
    return set(tokenize_ko(evidence_text))

KOREAN_STOPWORDS = {
    "ê·¸ë¦¬ê³ ","ê·¸ëŸ¬ë‚˜","í•˜ì§€ë§Œ","ë˜í•œ","ë˜","í˜¹ì€","ë˜ëŠ”","ë•Œë¬¸","ë•Œë¬¸ì—","ì´ëŠ”","ì´ëŸ°","ì´ëŸ¬í•œ","ê·¸","ê·¸ëŸ°","ê·¸ëŸ¬í•œ",
    "ê²ƒ","ì‚¬ì‹¤","ë¬¸ì œ","í•´ì„","ì„¤ëª…","ë™ì¼","ì¸ë¬¼","ì´ë¦„","ì—¬ëŸ¬","ê¸°ë¡","ë²ˆì—­","ì°¨ì´","ê²½ìš°","ë³¸ë¬¸","ì„±ê²½",
    "ì˜ë¯¸","ì‚¬ìš©","ì˜ˆ","ê°™ë‹¤","ìˆë‹¤","ì—†ë‹¤","ë¡œ","ì€","ëŠ”","ì´","ê°€","ì„","ë¥¼","ì—","ì—ì„œ","ê³¼","ì™€","ìœ¼ë¡œ",
    "ì—ê²Œ","ë³´ë‹¤","ê¹Œì§€","ë¶€í„°","ì²˜ëŸ¼","ì´ë‹¤","ì˜€ë‹¤","ì•„ë‹ˆë‹¤","ëœë‹¤","ë˜ë©°","ë˜ì–´","ë˜ì—ˆë‹¤","ë“±","ë”°ë¼ì„œ","ê·¸ëŸ¬ë¯€ë¡œ","ì¦‰",
    "ì „í†µì ìœ¼ë¡œ","ì •í™•íˆ","ëª…ì‹œ","ê·¼ê±°","ìš”ì•½"
}

def is_safe_summary(answer: str, evidence_text: str, coverage_threshold: float = 0.60) -> bool:
    """
    ì¦ê±° ì–´íœ˜ í¬í•¨ë¥ (coverage)ë¡œ ìš”ì•½ì˜ 'ê·¼ê±° ì¶©ì‹¤ì„±'ì„ í™•ì¸.
    coverage_thresholdë¥¼ ë‚®ì¶œìˆ˜ë¡ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ í—ˆìš© í­ì´ ì»¤ì§.
    """
    toks = tokenize_ko(answer)
    if not toks:
        return False
    allow = allowed_token_set(evidence_text)
    ok = [t for t in toks if (t in allow or t in KOREAN_STOPWORDS)]
    coverage = len(ok) / len(toks)
    if coverage < coverage_threshold:
        return False
    # ìˆ«ì/ê¸¸ì´ 3+ í† í°ì€ ì¦ê±° ì™¸ ìƒì„± ê¸ˆì§€
    for t in toks:
        if (re.search(r"\d", t) or len(t) >= 3) and (t not in allow and t not in KOREAN_STOPWORDS):
            return False
    if answer.strip().endswith("?"):
        return False
    return True

def _gen_with_ollama(system: str, user: str, temperature: float, num_predict: int):
    res = client.chat(
        model=OLLAMA_GEN_MODEL,
        messages=[{"role": "system", "content": system},
                  {"role": "user", "content": user}],
        options={"temperature": temperature, "num_predict": num_predict, "repeat_penalty": 1.15},
    )
    return (res.get("message", {}).get("content") or "").strip()

def safe_summarize(query: str, evidence_text: str, num_predict: int = 320, temperature: float = 0.15) -> str:
    """
    ì•ˆì „ ìš”ì•½(í‘œí˜„ì€ ìì—°ìŠ¤ëŸ½ê²Œ, ì‚¬ì‹¤ì€ ì¦ê±° ì•ˆì—ì„œë§Œ).
    """
    system = (
        "ë„ˆëŠ” í•œêµ­ì–´ RAG ë¹„ì„œë‹¤. ì•„ë˜ [ì¦ê±°] ë²”ìœ„ë¥¼ ì ˆëŒ€ ë²—ì–´ë‚˜ì§€ ë§ë¼.\n"
        "- ìˆ«ì, ê³ ìœ ëª…ì‚¬, ì „ë¬¸ìš©ì–´ëŠ” [ì¦ê±°]ì— ìˆëŠ” ê²ƒë§Œ ì‚¬ìš©í•œë‹¤.\n"
        "- í‘œí˜„ì€ ìì—°ìŠ¤ëŸ½ê²Œ ë°”ê¿” ë§í•´ë„ ëœë‹¤(íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ í—ˆìš©).\n"
        "- ì§ˆë¬¸/ë°˜ë¬¸, ê°íƒ„, ì¶”ì¸¡, ì‚¬ì¡± ê¸ˆì§€. í•œ ë‹¨ë½ì˜ ì„œìˆ í˜• ë‹µë§Œ ì¶œë ¥."
    )
    prompt = (
        f"[ì¦ê±°]\n{evidence_text}\n\n"
        f"[ì§ˆë¬¸]\n{query}\n\n"
        "í˜•ì‹: í•œ ë‹¨ë½, ì„œìˆ í˜•. ì¦ê±° ë°– ì‚¬ì‹¤ ê¸ˆì§€. í‘œí˜„ì€ ìì—°ìŠ¤ëŸ½ê²Œ."
    )
    return _gen_with_ollama(system, prompt, temperature=temperature, num_predict=num_predict)

def gentle_summarize(query: str, evidence_text: str, num_predict: int = 280, temperature: float = 0.22) -> str:
    """
    ê°€ë“œì— ê±¸ë ¸ì„ ë•Œ ì¬ì‹œë„ìš©: ë™ì¼ ì œì•½ì´ë˜ í†¤ì„ ë” ë¶€ë“œëŸ½ê²Œ.
    """
    system = (
        "ë„ˆëŠ” í•œêµ­ì–´ RAG ë¹„ì„œë‹¤. [ì¦ê±°]ë§Œ ê·¼ê±°ë¡œ ì‚¼ì•„ ê°„ê²°í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ìš”ì•½í•œë‹¤.\n"
        "- ì‚¬ì‹¤/ìˆ«ì/ëª…ì¹­ì€ ì¦ê±°ì—ì„œë§Œ ê°€ì ¸ì˜¨ë‹¤.\n"
        "- ë¶€ë“œëŸ¬ìš´ ì—°ê²°ì–´ë¥¼ í—ˆìš©í•˜ë˜ ê³¼ì¥/ì¶”ì¸¡ì€ ê¸ˆì§€.\n"
        "- í•œ ë‹¨ë½ìœ¼ë¡œ ì¶œë ¥."
    )
    prompt = f"[ì¦ê±°]\n{evidence_text}\n\n[ì§ˆë¬¸]\n{query}\n\ní˜•ì‹: í•œ ë‹¨ë½ ìš”ì•½."
    return _gen_with_ollama(system, prompt, temperature=temperature, num_predict=num_predict)

def answer_generic(query: str, context_md: str, emb_for_sent: OllamaEmbeddings,
                   summary_mode: str = "Safe Summary", num_predict: int = 320,
                   coverage_threshold: float = 0.55, base_temperature: float = 0.15) -> str:
    sents = build_sentence_pool(context_md, max_pool=160)
    top = rerank_sentences(query, sents, emb_for_sent, top_n=6, beta=0.40)
    if not top:
        return "ì»¨í…ìŠ¤íŠ¸ì— ì—†ìŒ"

    evidence_sents = [s for _, s, _, _ in top]
    evidence_text = " ".join(evidence_sents)

    if summary_mode == "Quotes only":
        return " ".join([s for s in evidence_sents if "?" not in s][:3])

    # 1ì°¨: ì•ˆì „ ìš”ì•½
    ans = safe_summarize(query, evidence_text, num_predict=num_predict, temperature=base_temperature)
    if not ans.strip().endswith("?") and is_safe_summary(ans, evidence_text, coverage_threshold=coverage_threshold):
        return ans

    # 2ì°¨: ë¶€ë“œëŸ¬ìš´ ì¬ìš”ì•½(íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆí­ ì‚´ì§â†‘)
    ans2 = gentle_summarize(query, evidence_text, num_predict=max(160, num_predict-40),
                            temperature=min(0.30, base_temperature + 0.05))
    if not ans2.strip().endswith("?") and is_safe_summary(ans2, evidence_text, coverage_threshold=max(0.40, coverage_threshold-0.05)):
        return ans2

    # 3ì°¨: ì¸ìš© í´ë°±
    quotes = [s for s in evidence_sents if "?" not in s][:3]
    return " ".join(quotes) if quotes else "ì»¨í…ìŠ¤íŠ¸ì— ì—†ìŒ"


# ========= Streamlit UI =========
st.set_page_config(page_title="Creation.kr Q&A (Chroma+Ollama RAG)", page_icon="ğŸ§­", layout="centered")
st.title("ğŸ¤– ì°½ì¡°ê³¼í•™ Q&A ì±—ë´‡ â€” Chroma DB ê¸°ë°˜ RAG (ì¼ë°˜í˜•Â·ë³µë¶™ ìµœì†Œí™” íŠœë‹)")

with st.sidebar:
    st.subheader("ê²€ìƒ‰/ìƒì„± ì„¤ì •")
    mode = st.radio(
        "Answer Mode",
        options=["Safe Summary (ê¶Œì¥)", "Quotes only", "Strict Substring"],
        index=0,
        help="Safe Summary: ì¦ê±° ê¸°ë°˜ ì•ˆì „ ìš”ì•½(ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„) / Quotes only: ì¦ê±° ë¬¸ì¥ ì¸ìš© / Strict: ì§§ì€ êµ¬ ë°œì·Œ"
    )
    fast_mode = st.toggle("âš¡ Fast Mode (ë” ë¹ ë¥¸ ì‘ë‹µ)", value=True, help="num_predictë¥¼ ë‚®ì¶”ê³  ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ")
    k = st.slider("Top-k", 2, 15, 5, 1)
    fetch_k = st.slider("Fetch-k (ë„“ê²Œ ê¸ê¸°)", 8, 64, 32, 4)
    ctx_limit = st.slider("Context ê¸¸ì´ ì œí•œ(ë¬¸ì ìˆ˜)", 2000, 9000, 4500, 500)
    alpha = st.slider("í•˜ì´ë¸Œë¦¬ë“œ ê°€ì¤‘ì¹˜ Î± (ì„ë² ë”© ë¹„ì¤‘)", 0.0, 1.0, 0.65, 0.05)
    mmr_lambda = st.slider("MMR Î» (ë‹¤ì–‘ì„± ê°€ì¤‘)", 0.1, 0.9, 0.55, 0.05)

    # ğŸ”§ ì¶”ê°€: ìš”ì•½ ììœ ë„ì™€ ìì—°ìŠ¤ëŸ¬ì›€ ì»¨íŠ¸ë¡¤
    paraphrase_level = st.select_slider(
        "ìš”ì•½ ììœ ë„(ìì—°ìŠ¤ëŸ¬ì›€)",
        options=["ë³´ìˆ˜ì ", "ë³´í†µ", "ììœ "],
        value="ë³´í†µ",
        help="ë†’ì¼ìˆ˜ë¡ í‘œí˜„ì´ ìì—°ìŠ¤ëŸ¬ì›Œì§€ì§€ë§Œ, ì¦ê±° ì–´íœ˜ ì¼ì¹˜ ë¹„ìœ¨ì€ ë‚®ì•„ì§ˆ ìˆ˜ ìˆìŒ"
    )
    if paraphrase_level == "ë³´ìˆ˜ì ":
        coverage_th = 0.60
        base_temp = 0.12
    elif paraphrase_level == "ììœ ":
        coverage_th = 0.45
        base_temp = 0.22
    else:  # ë³´í†µ
        coverage_th = 0.55
        base_temp = 0.15

    gen_tokens = 192 if fast_mode else 320
    st.caption("ìœ ì‚¬ë„/í‚¤ì›Œë“œ ì ìˆ˜ëŠ” ì•„ë˜ 'ì»¨í…ìŠ¤íŠ¸/ì¶œì²˜'ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# ë²¡í„° ìŠ¤í† ì–´ ì—°ê²°
try:
    store = get_store()
    st.success("Chroma ì—°ê²° ì„±ê³µ âœ… (ê¸°ì¡´ ë²¡í„° DB ì‚¬ìš©)")
    warn_if_embedding_mismatch(store)
except Exception as e:
    st.error(f"Chroma ì—°ê²° ì‹¤íŒ¨: {e}")
    st.stop()

# (ì˜µì…˜) CSV ë¡œë“œ (ì •í•© í™•ì¸ ìš©ë„)
try:
    if CSV_DEFAULT:
        df = load_csv(CSV_DEFAULT)
        _ = docs_for_bm25(df)
except Exception:
    pass  # CSV ì—†ì–´ë„ ë™ì‘

# ì„¸ì…˜ ìƒíƒœ (ì±„íŒ… ê¸°ë¡)
if "messages" not in st.session_state:
    st.session_state["messages"] = []

for msg in st.session_state["messages"]:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ì…ë ¥ & ì‘ë‹µ
user_q = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜, ì¼ë°˜í˜•)")
if user_q:
    st.session_state["messages"].append({"role": "user", "content": user_q})
    with st.chat_message("user"):
        st.markdown(user_q)

    with st.chat_message("assistant"):
        with st.spinner("ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì¤‘â€¦"):
            context_md, sources = retrieve_context(
                store, user_q, k=k, fetch_k=fetch_k, ctx_char_limit=ctx_limit, alpha=alpha,
                use_mmr=True, mmr_lambda=mmr_lambda
            )

            if not context_md.strip():
                st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì»¬ë ‰ì…˜/ì„ë² ë”© ëª¨ë¸ ì¼ì¹˜ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                answer_md = "ì»¨í…ìŠ¤íŠ¸ì— ì—†ìŒ"
                sources = []
            else:
                emb_for_sent = OllamaEmbeddings(model=OLLAMA_EMBED_MODEL, base_url=OLLAMA_BASE)

                if mode == "Strict Substring":
                    STRICT_SYSTEM = (
                        "ë„ˆëŠ” í•œêµ­ì–´ RAG ë¹„ì„œë‹¤. ë°˜ë“œì‹œ [ì»¨í…ìŠ¤íŠ¸]ì— ë“¤ì–´ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•œë‹¤. "
                        "ì •ë‹µì€ [ì»¨í…ìŠ¤íŠ¸]ì— ë¬¸ì ê·¸ëŒ€ë¡œ ì¡´ì¬í•˜ëŠ” ì§§ì€ êµ¬(2~40ì)ë§Œ ì¶œë ¥. "
                        "ì—­ì§ˆë¬¸/ì‚¬ì¡± ê¸ˆì§€. ì—†ìœ¼ë©´ 'ì»¨í…ìŠ¤íŠ¸ì— ì—†ìŒ'ë§Œ ì¶œë ¥."
                    )
                    prompt = f"[ì»¨í…ìŠ¤íŠ¸]\n{context_md}\n\n[ì§ˆë¬¸]\n{user_q}\n\ní˜•ì‹: ì •ë‹µ êµ¬(2~40ì)ë§Œ ì¶œë ¥."
                    res = client.chat(
                        model=OLLAMA_GEN_MODEL,
                        messages=[{"role": "system", "content": STRICT_SYSTEM},
                                  {"role": "user", "content": prompt}],
                        options={"temperature": 0.0, "top_p": 0.1, "repeat_penalty": 1.1, "num_predict": 128},
                    )
                    a = (res.get("message", {}).get("content") or "").strip()
                    if normalize_ko(a) and normalize_ko(a) in normalize_ko(context_md) and 2 <= len(a) <= 40:
                        answer_md = a
                    else:
                        answer_md = "ì»¨í…ìŠ¤íŠ¸ì— ì—†ìŒ"

                elif mode == "Quotes only":
                    # ì¸ìš©ë§Œ ì¶œë ¥
                    sents = build_sentence_pool(context_md, max_pool=160)
                    top = rerank_sentences(user_q, sents, emb_for_sent, top_n=6, beta=0.40)
                    quotes = [s for _, s, _, _ in top if "?" not in s][:3]
                    answer_md = " ".join(quotes) if quotes else "ì»¨í…ìŠ¤íŠ¸ì— ì—†ìŒ"

                else:  # Safe Summary (ê¶Œì¥)
                    answer_md = answer_generic(
                        user_q, context_md, emb_for_sent,
                        summary_mode="Safe Summary", num_predict=gen_tokens,
                        coverage_threshold=coverage_th, base_temperature=base_temp
                    )

        # 1) ë³¸ë¬¸ ë‹µë³€
        st.markdown(answer_md)

        # 2) ì›ë¬¸ URL ëª©ë¡
        if sources:
            st.markdown("\n\n**ì›ë¬¸ ë§í¬**")
            seen = set()
            for s in sources:
                url = s.get("url", "")
                title = s.get("title", "") or url
                if url and url not in seen:
                    st.markdown(f"- [{title}]({url})")
                    seen.add(url)

        # 3) ì»¨í…ìŠ¤íŠ¸/ìœ ì‚¬ë„ëŠ” expanderë¡œ
        with st.expander("ğŸ” ì‚¬ìš©í•œ ì»¨í…ìŠ¤íŠ¸ / ì¶œì²˜ (ìœ ì‚¬ë„Â·í‚¤ì›Œë“œ í¬í•¨)"):
            st.markdown(context_md if context_md else "_(ë¹„ì–´ ìˆìŒ)_")

    st.session_state["messages"].append({"role": "assistant", "content": answer_md})
